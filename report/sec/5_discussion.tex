\section{Discussion}
\label{sec:discussion}

As mentioned previously, one of the primary objectives of this study was to perform a comparative analysis between our Gaussian splat-based ResNet autoencoder and a more conventional pixel-based ResNet autoencoder. Both approaches, utilizing the same model architecture, were evaluated in terms of compression efficiency and reconstruction performance.

For our model, we trained a ResNet architecture on five separate models, each corresponding to a distinct Gaussian splat parameter, based on our newly created CIFAR-10 dataset of Gaussian splats. In contrast, for the conventional approach, we employed the ResNet-18 model implementation \footnote{\url{https://github.com/eleannavali/resnet-18-autoencoder}} and trained it on the original CIFAR-10 dataset.

Figure \ref{TODO} illustrates the comparison between the two approaches using random test images from each class. The comparison clearly demonstrates that our Gaussian splat-based model yields slightly better results in terms of reconstruction quality. Quantitative analysis, such as the Structural Similarity Index (SSIM), further supports this finding, with the SSIM difference between the original and our model being TODO, while the difference between the original and the conventional model is TODO.

% TODO - Insert image (resnet_comparison_test)

However, when comparing the compression ratios between both approaches, the conventional method proves to be more efficient, achieving a compression ratio of TODO, while our model achieves a compression ratio of TODO. This indicates that while our approach results in higher-quality reconstructions, it is less efficient in terms of compression, and, in fact, increases the image size. If the compression ratio exceeds 1, we would need to further investigate whether this is a characteristic of our method or an inherent trade-off for achieving better image quality.

\subsection{Future Work}

Finally, we have identified several potential areas for future research and applications where this approach could be utilized:

\begin{itemize}
    \item \textbf{Exploring the effect of latent space size on reconstruction error:} It would be valuable to investigate how the reconstruction error grows as the size of the latent space increases. Understanding this relationship could help optimize the balance between compression efficiency and reconstruction quality.
    
    \item \textbf{Loss function refinement:} An interesting avenue of investigation would be to explore whether the autoencoder should be trained by minimizing the loss based only on the Gaussian splats, or if it would be beneficial to enforce consistency between the rendered splat and the original image. This could potentially improve the reconstruction accuracy or reduce artifacts.
    
    \item \textbf{Implementation of Hierarchical Perceiver (HiP):} Incorporating HiP, as discussed in \cite{carreira2022hierarchicalp}, could be an interesting future work. The hierarchical nature of HiP may provide better structure to the encoding process, enhancing performance on Gaussian splats.
    
    \item \textbf{Latent space classification:} Exploring the possibility of using the latent space for downstream tasks, such as classification, could open up new applications for Gaussian splat-based representations.
    
    \item \textbf{Generative modeling on the latent space:} Building a generative model (e.g., Generative Adversarial Networks or Stable Diffusion) on top of the latent space could allow us to generate new images or 3D scenes from the compressed representations, facilitating content creation or data augmentation tasks.
    
    \item \textbf{Gaussian splat generation:} Developing a model capable of generating new Gaussian splats (e.g., a Variational Autoencoder) would be an intriguing direction. This could lead to more sophisticated image synthesis techniques, allowing us to generate realistic images directly from the splat parameters.
    
    \item \textbf{Masked Autoencoders (MAE) for smaller images:} Finally, investigating the application of MAE on smaller images could help determine if this approach could provide more efficient representations and faster processing times, especially in cases where the dataset is constrained in size.
\end{itemize}
