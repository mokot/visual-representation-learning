{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to always autoreload modules and avoid restarting the kernel each time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a hack to allow importing from the parent directory\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from models.gaussian_image_trainer import GaussianImageTrainer\n",
    "from configs import Config\n",
    "from utils.search_space import is_valid_combination\n",
    "import torchvision\n",
    "from constants import CIFAR10_TRANSFORM\n",
    "from utils.data import create_default_image\n",
    "from utils import visualize_tensor\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Only used one image for the first tuning\n",
    "dataset = torchvision.datasets.CIFAR10(root=\"./data\", download=True)\n",
    "# cifar_image = CIFAR10_TRANSFORM(dataset[2][0])\n",
    "cifar_imgs = [CIFAR10_TRANSFORM(dataset[i][0]) for i in range(4)]\n",
    "visualize_tensor(cifar_imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 03:18:46,129] A new study created in RDB with name: study-w-grid-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in RDB with name: study-w-grid-3\n",
      "A new study created in RDB with name: study-w-grid-3\n"
     ]
    }
   ],
   "source": [
    "# New experiments with some key learnings:]\n",
    "# 1. LR 0.01 seems to be the best.\n",
    "# 2. Max Steps 2000 is better than 1000, larger values are not much better.\n",
    "# 3. Init Type: KNN is the best, Grid is still interesting for other reasons (AE), don't use random. \n",
    "# 4. Model Type: 2dgs is superior\n",
    "# 5. More points is better, but 32x32 is already very good.\n",
    "# 6. Init Scale: 2.0 -> 3.0 is also good\n",
    "# 7. Init Opacity: 0.5 -> when Opacity Regularization is None, other init opacities also worked well\n",
    "# 8. Opacity Regularization: None -> very clear winner\n",
    "# 9. Scale Regularization: 0.1\n",
    "# 10. Extent: 1.0 -> 2.0 and 4.0 are very similar also, could pick any\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name_grid = \"study-w-grid-3\"  # Unique identifier of the study.\n",
    "storage_name_grid = \"sqlite:///{}.db\".format(study_name_grid)\n",
    "study_w_grid = optuna.create_study(study_name=study_name_grid, storage=storage_name_grid, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_w_grid_3(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): The trial object used to suggest hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation metric to minimize (e.g., validation loss).\n",
    "    \"\"\"\n",
    "    # Conditional sampling\n",
    "    # group_optimization = trial.suggest_categorical(\"group_optimization\", [True, False])\n",
    "    # strategy = None\n",
    "    group_optimization = True # For now, as having default strategy was causing an error\n",
    "    if not group_optimization:\n",
    "        strategy = trial.suggest_categorical(\"strategy\", [\"default\", \"mcmc\"])\n",
    "\n",
    "    # Hyperparameter suggestions\n",
    "    loss_weights = [\n",
    "        1/3,\n",
    "        1/3,\n",
    "        1/3,\n",
    "    ]\n",
    "\n",
    "    scale_regularization = 0.1\n",
    "    opacity_regularization = None\n",
    "\n",
    "    # init_type = trial.suggest_categorical(\"init_type\", [\"grid\", \"knn\"])\n",
    "    init_type = \"grid\"  # Focus on the option with a fixed grid for this study\n",
    "    num_points = 32*32\n",
    "    extent = trial.suggest_categorical(\"extent\", [1.0, 2.0, 4.0])\n",
    "    init_scale = 2.0\n",
    "    init_opacity = 0.5\n",
    "\n",
    "    max_steps = 1000\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    model_type = \"2dgs\"  # Left out \"2dgs-inria\" as it needed further dependencies\n",
    "    bilateral_grid = False\n",
    "\n",
    "    image_index = trial.suggest_categorical(\"image_index\", [0, 1, 2, 3])\n",
    "\n",
    "    # Added these hyperparameters:\n",
    "    sparse_gradient = trial.suggest_categorical(\"sparse_gradient\", [True, False])\n",
    "    # normal_loss_weight = trial.suggest_float(\"normal_loss_weight\", 0.1, 1.0, log=True)\n",
    "    # distortion_loss_weight = trial.suggest_float(\"distortion_loss_weight\", 0.1, 1.0, log=True)\n",
    "    sh_degree = trial.suggest_categorical(\"sh_degree\", [0, 1, 2, 3])  # I guess this is correct as it seems to behave as a boolean\n",
    "\n",
    "    if not is_valid_combination({\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"loss_weights\": loss_weights,\n",
    "        \"group_optimization\": group_optimization,\n",
    "        # \"strategy\": strategy,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"model_type\": model_type,\n",
    "        \"bilateral_grid\": bilateral_grid,\n",
    "        # Added these:\n",
    "        \"sparse_gradient\": sparse_gradient,\n",
    "        #\"normal_loss_weight\": normal_loss_weight,\n",
    "        #\"distortion_loss_weight\": distortion_loss_weight,\n",
    "        \"sh_degree\": sh_degree,\n",
    "    }, GaussianImageTrainer):\n",
    "        raise optuna.exceptions.TrialPruned(\"Invalid hyperparameter combination\")\n",
    "\n",
    "    # Create Config object\n",
    "    cfg = Config(\n",
    "        save_results=False,\n",
    "        save_logs=False,\n",
    "        seed=42,\n",
    "        image=cifar_imgs[image_index],  # Replace with actual ground truth image tensor\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        loss_weights=loss_weights,\n",
    "        init_type=init_type,\n",
    "        num_points=num_points,\n",
    "        init_scale=init_scale,\n",
    "        init_opacity=init_opacity,\n",
    "        scale_regularization=scale_regularization,\n",
    "        opacity_regularization=opacity_regularization,\n",
    "        extent=extent,\n",
    "        group_optimization=group_optimization,\n",
    "        # Added these:\n",
    "        sparse_gradient=sparse_gradient,\n",
    "        #normal_loss_weight=normal_loss_weight,\n",
    "        #distortion_loss_weight=distortion_loss_weight,\n",
    "        sh_degree=sh_degree,\n",
    "        # strategy=strategy,\n",
    "        model_type=model_type,\n",
    "        # bilateral_grid=bilateral_grid,\n",
    "        bilateral_grid=False\n",
    "    )\n",
    "\n",
    "    # Initialize and train the model\n",
    "    trainer = GaussianImageTrainer(cfg)\n",
    "    result = trainer.train()\n",
    "\n",
    "    # Evaluate generated image quality (e.g., L1 loss with ground truth)\n",
    "    generated_image = result.cpu()  # Replace with actual rendered output\n",
    "    ground_truth_image = cfg.image.cpu()\n",
    "    # evaluation_metric = torch.nn.functional.l1_loss(generated_image, ground_truth_image)  # Old version\n",
    "    evaluation_metric = trainer.l1_loss * 1/3 + trainer.mse_loss * 1/3 + trainer.ssim_loss * 1/3  # Simply take average as loss\n",
    "\n",
    "    return evaluation_metric.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Number of Gaussians: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.049 (L1: 0.014, MSE: 0.000, SSIM: 0.058): 100%|\u001b[36m██████\u001b[0m| 1000/1000 [00:06<00:00, 159.47step/s]\u001b[0m\n",
      "[I 2025-01-09 03:18:57,575] Trial 0 finished with value: 0.023919718340039253 and parameters: {'extent': 2.0, 'image_index': 3, 'sparse_gradient': False, 'sh_degree': 0}. Best is trial 0 with value: 0.023919718340039253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.048982612788677216\n",
      "Total Time: Rasterization: 0.973s, Backward: 2.486s\n",
      "Trial 0 finished with value: 0.023919718340039253 and parameters: {'extent': 2.0, 'image_index': 3, 'sparse_gradient': False, 'sh_degree': 0}. Best is trial 0 with value: 0.023919718340039253.\n",
      "Trial 0 finished with value: 0.023919718340039253 and parameters: {'extent': 2.0, 'image_index': 3, 'sparse_gradient': False, 'sh_degree': 0}. Best is trial 0 with value: 0.023919718340039253.\n",
      "Model initialized. Number of Gaussians: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|\u001b[36m                                                 \u001b[0m| 0/1000 [00:00<?, ?step/s]\u001b[0m\n",
      "[W 2025-01-09 03:18:58,974] Trial 1 failed with parameters: {'extent': 1.0, 'image_index': 2, 'sparse_gradient': True, 'sh_degree': 0} because of the following error: RuntimeError('Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/user/22473/ipykernel_2101078/1871683771.py\", line 95, in objective_w_grid_3\n",
      "    result = trainer.train()\n",
      "  File \"/home/h/harjesruiloba/Projects/visual-representation-learning/models/gaussian_image_trainer.py\", line 568, in train\n",
      "    start = time.time()\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 failed with parameters: {'extent': 1.0, 'image_index': 2, 'sparse_gradient': True, 'sh_degree': 0} because of the following error: RuntimeError('Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/user/22473/ipykernel_2101078/1871683771.py\", line 95, in objective_w_grid_3\n",
      "    result = trainer.train()\n",
      "  File \"/home/h/harjesruiloba/Projects/visual-representation-learning/models/gaussian_image_trainer.py\", line 568, in train\n",
      "    start = time.time()\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)\n",
      "Trial 1 failed with parameters: {'extent': 1.0, 'image_index': 2, 'sparse_gradient': True, 'sh_degree': 0} because of the following error: RuntimeError('Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/user/22473/ipykernel_2101078/1871683771.py\", line 95, in objective_w_grid_3\n",
      "    result = trainer.train()\n",
      "  File \"/home/h/harjesruiloba/Projects/visual-representation-learning/models/gaussian_image_trainer.py\", line 568, in train\n",
      "    start = time.time()\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/h/harjesruiloba/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-09 03:18:59,011] Trial 1 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 failed with value None.\n",
      "Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/user/22473/ipykernel_2101078/340592419.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudy_w_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_w_grid_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/user/22473/ipykernel_2101078/1871683771.py\u001b[0m in \u001b[0;36mobjective_w_grid_3\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Initialize and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianImageTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Evaluate generated image quality (e.g., L1 loss with ground truth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/visual-representation-learning/models/gaussian_image_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m~/miniforge3/envs/nerfstudio/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sparse division requires a scalar or zero-dim dense tensor divisor (got shape [1024, 1] for divisor)"
     ]
    }
   ],
   "source": [
    "study_w_grid.optimize(objective_w_grid_3, n_trials=1000, n_jobs=1, timeout=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
